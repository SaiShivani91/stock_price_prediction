# -*- coding: utf-8 -*-
"""Copy of Capstone final 99999.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1U9aMGTo4BCwJvEuRgrMSQF8m9Ux6rfBF
"""

!pip install yfinance

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
import os
from subprocess import check_output
import seaborn as sns
import matplotlib.pyplot as plt
import warnings
from pandas.plotting import lag_plot
from pandas import datetime
from statsmodels.tsa.arima.model import ARIMA
from sklearn.metrics import mean_squared_error
sns.set_style('whitegrid')
plt.style.use("fivethirtyeight")

import datetime

import yfinance as yf

# Define the ticker symbol for Microsoft
ticker_symbol = "MSFT"

# Define the start and end dates for the data you want
start_date = "2018-01-01"
end_date = "2023-01-01"

# Use yfinance to download the data
data = yf.download(ticker_symbol, start=start_date, end=end_date)

round(data.head(), 3)

round(data.tail(), 3)

"""EDA"""

print(data.head())
print(data.tail())

data.shape

"""Descriptive Statistics about the Data"""

data.describe().T

data.info()





data[['Close']].plot()
plt.title("Microsoft")
plt.show()

data.plot(figsize=(15,7), linewidth=5, fontsize=20)
plt.xlabel('Year', fontsize=20)

data['Adj Close'].plot()
plt.ylabel('Adj Close')
plt.xlabel(None)
plt.title(f"Closing Price of Microsoft")

plt.tight_layout()

data['Volume'].plot()
plt.ylabel('Volume')
plt.xlabel(None)
plt.title(f"Sales Volume for Microsoft")

plt.tight_layout()

# TRend
dr = data['Close'].cumsum()
dr.plot()
plt.title('Microsoft Cumulative Returns')
plt.legend()

plt.figure(figsize=(5,5))
lag_plot(data['Close'], lag=5)
plt.title('Microsoft Autocorrelation plot')

# Distribution of the closing price
plt.figure(figsize=(8, 6))
sns.histplot(data['Close'], kde=True, color='b')
plt.title('Distribution of Closing Price')
plt.xlabel('Closing Price')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

# Pairplot for selected columns
selected_columns = ['Open', 'High', 'Low', 'Close', 'Volume']
sns.pairplot(data[selected_columns])
plt.suptitle('Pairplot of Selected Columns', y=1.02)
plt.show()

# Correlation heatmap
correlation_matrix = data[selected_columns].corr()
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Heatmap')

train_data, test_data = data[0:int(len(data)*0.8)], data[int(len(data)*0.8):]
plt.figure(figsize=(12,7))
plt.title('Microsoft Stock Prices')
plt.xlabel('Dates')
plt.ylabel('Prices')
plt.plot(data['Close'], 'blue', label='Training Data')
plt.plot(test_data['Close'], 'red', label='Testing Data')

"""Stationarity test"""

import matplotlib.pyplot as plt
from statsmodels.tsa.stattools import adfuller

def stationarity(col):
    res = adfuller(col.dropna())
    print("Test Statistic:", res[0])
    print("P-Value:", res[1])
    if res[1] < 0.05:
        print("Stationary")
    else:
        print("Non-Stationary")

stationarity(data['Close'])

diff_1_close = data['Close'].diff().dropna()
diff_1_close.plot(figsize=(15,8))

stationarity(diff_1_close)

!pip install pmdarima

from statsmodels.tsa.stattools import acf, pacf
from pmdarima import auto_arima
from statsmodels.tsa.arima.model import ARIMA
from sklearn.metrics import mean_absolute_error, mean_squared_error

from statsmodels.graphics.tsaplots import plot_pacf, plot_acf

def pacf(col):
    fig, (ax1,ax2) = plt.subplots(1,2, figsize=(15,5))
    ax1.plot(col)
    ax1.set_title('The Column Diff')
    plot_pacf(col, method='ywm', ax=ax2);


def acf(col):
    fig, (ax1,ax2) = plt.subplots(1,2, figsize=(15,5))
    ax1.plot(col)
    ax1.set_title('The Column Diff')
    plot_acf(col, ax=ax2);

plot_acf(data['Close'].diff().dropna())

pacf(diff_1_close)
acf(diff_1_close)

from statsmodels.tsa.arima.model import ARIMA
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error


def arima_train_and_plot(data, col_name, Ntest, p, d, q):
    #split data to train and test sets based on N_test value
    train = data.iloc[:-Ntest]
    test = data.iloc[-Ntest:]
    train_idx = data.index <= train.index[-1]
    test_idx = data.index > train.index[-1]

    #Define and fit the arima model
    arima = ARIMA(train[col_name], order=(p,d,q))
    arima_res = arima.fit()

    #plot the real values of stock prices
    fig, ax= plt.subplots(figsize=(15,8))
    ax.plot(data[col_name], label='Actual return')

    #plot the fitted values of model (in sample data predicted values)
    train_pred = arima_res.fittedvalues
    ax.plot(train.index, train_pred, color='green', label='fitted')

    #plot the forecast values of model (out of sample data predicted values)
    prediction_res = arima_res.get_forecast(Ntest)
    conf_int = prediction_res.conf_int()
    #lower and upper limits of prediction
    lower, upper = conf_int[conf_int.columns[0]], conf_int[conf_int.columns[1]]
    forecast = prediction_res.predicted_mean
    ax.plot(test.index, forecast, label='forecast')
    ax.fill_between(test.index, lower, upper, color='red', alpha=0.3)
    ax.legend()

    #evaluating the model using RMSE and MAE metrics
    y_true = test[col_name].values
    rmse = np.sqrt(mean_squared_error(y_true,forecast))
    mae = mean_absolute_error(y_true,forecast)

    return rmse, mae

rmse, mae = arima_train_and_plot(data, 'Close', Ntest=252, p=1, d=0, q=1)
print('Root Mean Squared Error: ', rmse)
print('Mean Absolute Error: ', mae)

"""Auto ARIMA"""

import pmdarima as pm
from pmdarima.arima.utils import ndiffs

d_val = ndiffs(data['Close'], test='adf')
print('Arima D-value:', d_val)

Ntest = 252
train = data.iloc[:-Ntest]
test = data.iloc[-Ntest:]
train_idx = data.index <= train.index[-1]
test_idx = data.index > train.index[-1]

#Define auto-arima to find best model
model = pm.auto_arima(train['Close'],
                      d = d_val,
                      start_p = 0,
                      max_p = 10,
                      start_q = 0,
                      max_q = 10,
                      stepwise=False,
                      max_order=252,
                      trace=True)

model.get_params()

def plot_result(model, data, col_name, Ntest):

    params = model.get_params()
    d = params['order'][1]

    #In sample data prediction
    train_pred = model.predict_in_sample(start=d, end=-1)
    #out of sample prediction
    test_pred, conf = model.predict(n_periods=Ntest, return_conf_int=True)

    #plotting real values, fitted values and prediction values
    fig, ax= plt.subplots(figsize=(15,8))
    ax.plot(data[col_name].index, data[col_name], label='Actual Values')
    ax.plot(train.index[d:], train_pred, color='green', label='Fitted Values')
    ax.plot(test.index, test_pred, label='Forecast Values')
    ax.fill_between(test.index, conf[:,0], conf[:,1], color='red', alpha=0.3)
    ax.legend()

    #evaluating the model using RMSE and MAE metrics
    y_true = test[col_name].values
    rmse = np.sqrt(mean_squared_error(y_true,test_pred))
    mae = mean_absolute_error(y_true,test_pred)

    return rmse, mae

rmse , mae = plot_result(model, data, 'Close', Ntest=252)
print('Root Mean Squared Error: ', rmse)
print('Mean Absolute Error: ', mae)

"""SARIMAX"""

from statsmodels.tsa.statespace.sarimax import SARIMAX
from statsmodels.tsa.stattools import acf, pacf
from sklearn.metrics import mean_absolute_error, mean_squared_error

# Function to plot autocorrelation and partial autocorrelation graphs
def plot_acf_pacf(series):
    lag_acf = acf(series, nlags=20)
    lag_pacf = pacf(series, nlags=20, method='ols')

    # Plot ACF
    plt.subplot(121)
    plt.plot(lag_acf)
    plt.axhline(y=0, linestyle='--', color='gray')
    plt.axhline(y=-1.96/np.sqrt(len(series)), linestyle='--', color='gray')
    plt.axhline(y=1.96/np.sqrt(len(series)), linestyle='--', color='gray')
    plt.title('Autocorrelation Function')

    # Plot PACF
    plt.subplot(122)
    plt.plot(lag_pacf)
    plt.axhline(y=0, linestyle='--', color='gray')
    plt.axhline(y=-1.96/np.sqrt(len(series)), linestyle='--', color='gray')
    plt.axhline(y=1.96/np.sqrt(len(series)), linestyle='--', color='gray')
    plt.title('Partial Autocorrelation Function')

    plt.show()

# Plot autocorrelation and partial autocorrelation graphs
plot_acf_pacf(data['Close'])

# Find the optimal SARIMAX model parameters
# You may need to adjust the parameters based on the ACF and PACF plots
order = (1, 0, 1)  # (p, d, q)
seasonal_order = (1, 0, 1, 12)  # (P, D, Q, s)

# Split the data into training and testing sets
train_size = int(len(data) * 0.8)
train, test = data['Close'][:train_size], data['Close'][train_size:]

# Fit the SARIMAX model with the optimal parameters on the training data
sarimax_model = SARIMAX(train, order=order, seasonal_order=seasonal_order)
sarimax_result = sarimax_model.fit(disp=False)

# Make predictions on the test set
predictions = sarimax_result.get_forecast(steps=len(test))
predicted_values = predictions.predicted_mean

# Calculate Mean Absolute Error (MAE)
mae = mean_absolute_error(test, predicted_values)
print(f'Mean Absolute Error (MAE): {mae}')

# Calculate Root Mean Square Error (RMSE)
rmse = np.sqrt(mean_squared_error(test, predicted_values))
print(f'Root Mean Square Error (RMSE): {rmse}')

plt.figure(figsize=(12,7))
plt.plot(train, 'green', color='blue', label='Training Data')
plt.plot(test.index, predicted_values, color='yellow', marker='o', linestyle='dashed',
         label='Predicted Price')
plt.plot(test.index, test, color='red', label='Actual Price')
plt.title('Microsoft Prices Prediction')
plt.xlabel('Dates')
plt.ylabel('Prices')

"""Random Forest"""

import warnings
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
warnings.filterwarnings("ignore")

#define the feature columns and target column
features = ['Open', 'High', 'Low', 'Adj Close', 'Volume']
target = 'Close'

#train/test split
x=data[features]
y=data[target]
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

data.tail(252)

#train/test data plotting
train=data.loc[data.index<'01-01-2022']
test=data.loc[data.index>='31-12-2021']

fig,ax=plt.subplots(figsize=(15,5))
train.plot(ax = ax, label='Training Set', color = 'blue')
test.plot(ax = ax, label='Test Set', color = 'red')
ax.axvline('01-01-2022',color='black')
ax.legend(['Train','Test'])

plt.show()

# create Random Forest Regressor object
rf = RandomForestRegressor(n_estimators=100, max_depth=5, min_samples_split=2, min_samples_leaf=1, max_features='sqrt', bootstrap=True)

# Fitting RF Regression to the Training set
rf.fit(x_train, y_train)

# Predicting the Test set results
y_pred = rf.predict(x_test)

#evaluate model's performance on train data
predict = rf.predict(x_train)
r2 = r2_score(y_train, predict)
mse = mean_squared_error(y_train, predict)
rmse = mean_squared_error(y_train, predict, squared=False)
mae = mean_absolute_error(y_train, predict)
print("R2 Score:", r2)
print("Mean Squared Error:", mse)
print("Root Mean Squared Error:", rmse)
print("Mean Absolute Error:", mae)

#evaluate model's performance on test data
r2 = r2_score(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = mean_squared_error(y_test, y_pred, squared=False)
mae = mean_absolute_error(y_test, y_pred)
print("R2 Score:", r2)
print("Mean Squared Error:", mse)
print("Root Mean Squared Error:", rmse)
print("Mean Absolute Error:", mae)

#Predict close price on test OHL(open, high, low)
#predictions for data
test_ohl=test[features]
y_pred = rf.predict(test_ohl)

#plotting
plt.rcParams["figure.figsize"] = (20,10)

plt.plot(test.index,test[target], color = 'red', label="Actual Data")
plt.plot(test.index,y_pred, color='blue', label="Prediction")

plt.title('MSFT Stock Prices - Random Forest')
plt.xlabel('Days')
plt.ylabel('Close Price')
plt.legend()
plt.show()

"""XG Boost"""

from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error
from xgboost import XGBRegressor
import matplotlib.pyplot as plt

xgboost = XGBRegressor(objective='reg:squarederror', n_estimators=100, learning_rate=0.1, max_depth=3)
xgboost.fit(x_train, y_train)

y_pred = xgboost.predict(x_test)

#evaluate model's performance on train data
predict = xgboost.predict(x_train)
r2 = r2_score(y_train, predict)
mse = mean_squared_error(y_train, predict)
rmse = mean_squared_error(y_train, predict, squared=False)
mae = mean_absolute_error(y_train, predict)
print("R2 Score:", r2)
print("Mean Squared Error:", mse)
print("Root Mean Squared Error:", rmse)
print("Mean Absolute Error:", mae)

np.round(y_pred[-10:-1], 2)

round(data['Close'].tail(10), 3)

#Predict close price on test OHL(open, high, low)
#predictions for data
test_ohl=test[features]
y_pred = xgboost.predict(test_ohl)

#plotting
plt.rcParams["figure.figsize"] = (20,10)

plt.plot(test.index,test[target], color = 'red', label="Actual Data")
plt.plot(test.index,y_pred, color='blue', label="Prediction")

plt.title('MSFT Stock Prices - XGBoost')
plt.xlabel('Days')
plt.ylabel('Close Price')
plt.legend()
plt.show()



"""LSTM"""

from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense

# Use the closing prices for prediction
data = data[['Close']]
data['Date'] = data.index

# Normalize the data
scaler = MinMaxScaler(feature_range=(0, 1))
data_scaled = scaler.fit_transform(data['Close'].values.reshape(-1, 1))

# Function to create time series data
def create_time_series_data(data, look_back=1):
    X, y = [], []
    for i in range(len(data)-look_back):
        X.append(data[i:(i+look_back), 0])
        y.append(data[i+look_back, 0])
    return np.array(X), np.array(y)

# Create time series data with a look back of 10 days
look_back = 15
X_lstm, y_lstm = create_time_series_data(data_scaled, look_back)

# Split the data into training and testing sets
train_size = int(len(X_lstm) * 0.8)
X_train_lstm, X_test_lstm = X_lstm[:train_size], X_lstm[train_size:]
y_train_lstm, y_test_lstm = y_lstm[:train_size], y_lstm[train_size:]

# Reshape the data for LSTM input (samples, time steps, features)
X_train_lstm = np.reshape(X_train_lstm, (X_train_lstm.shape[0], X_train_lstm.shape[1], 1))
X_test_lstm = np.reshape(X_test_lstm, (X_test_lstm.shape[0], X_test_lstm.shape[1], 1))

# Build the LSTM model
model_lstm = Sequential()
model_lstm.add(LSTM(50, input_shape=(look_back, 1)))
model_lstm.add(Dense(1))
model_lstm.compile(optimizer='adam', loss='mean_squared_error')

# Train the model
model_lstm.fit(X_train_lstm, y_train_lstm, epochs=50, batch_size=1, verbose=2)

# Make predictions on the test set
y_pred_lstm_scaled = model_lstm.predict(X_test_lstm)
y_pred_lstm = scaler.inverse_transform(y_pred_lstm_scaled)

# Inverse transform the actual values for evaluation
y_test_lstm_orig = scaler.inverse_transform(y_test_lstm.reshape(-1, 1))

# Evaluate the model using RMSE and MAE
rmse_lstm = np.sqrt(mean_squared_error(y_test_lstm_orig, y_pred_lstm))
mae_lstm = mean_absolute_error(y_test_lstm_orig, y_pred_lstm)

print(f'Root Mean Squared Error (RMSE) - LSTM: {rmse_lstm}')
print(f'Mean Absolute Error (MAE) - LSTM: {mae_lstm}')

# Plotting the actual vs predicted values on the test set
plt.figure(figsize=(14, 7))
plt.plot(y_test_lstm_orig, label='Actual', color='blue')
plt.plot(y_pred_lstm, label='LSTM Prediction', color='red')
plt.title('Microsoft Stock Price Prediction using LSTM')
plt.xlabel('Days')
plt.ylabel('Closing Price (USD)')
plt.legend()
plt.show()

# Plotting the actual vs predicted values on the test set
plt.figure(figsize=(14, 7))
plt.plot(y_test_lstm_orig, label='Actual', color='blue')
plt.plot(y_pred_lstm, label='LSTM Prediction', color='red')
plt.title('Microsoft Stock Price Prediction using LSTM')
plt.xlabel('Days')
plt.ylabel('Closing Price (USD)')
plt.legend()
plt.show()

